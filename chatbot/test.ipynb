{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to generate answer based on user questions.\n",
    "Input variables\n",
    "1. question\n",
    "2. Flag if include prompt template\n",
    "3. Flag if need defined structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import json \n",
    "\n",
    "sysmessage = \"\"\"\n",
    "You are a company information extractor.\n",
    "You will receive questions from users, give detailed answers (include data if available) based on the below context, and generate 3 related questions that users might be interested in.\n",
    "Answer I don't know if the information is not in the context.\n",
    "\"\"\"\n",
    "\n",
    "# Specify the directory for persistent storage\n",
    "persist_directory = os.path.dirname(os.getcwd()) + '\\\\data\\\\chroma\\\\vietcap_reports'\n",
    "embedding = OpenAIEmbeddings()\n",
    "# Initialize Chroma with embeddings and directory\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Answer(BaseModel):\n",
    "    answer_: str = Field(description=\"answer of the user's query\")\n",
    "    related_q1: str = Field(description=\"related question no.1\")\n",
    "    related_q2: str = Field(description=\"related question no.2\")\n",
    "    related_q3: str = Field(description=\"related question no.3\")\n",
    "        \n",
    "def qachatbot(question, include_prompt=False, structured_result=False):\n",
    "    if not include_prompt and structured_result:\n",
    "        raise Exception(\"include prompt if want to have structured answer\")\n",
    "    #---------------------------------------------------#\n",
    "    # Initialize OpenAI embeddings\n",
    "    openai.api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "    # Initialize ChatOpenAI with temperature set to 0\n",
    "    llm = ChatOpenAI(temperature=0)\n",
    "    \n",
    "    #---------------------------------------------------#\n",
    "    if include_prompt and structured_result:\n",
    "        template = sysmessage + \"\"\"\n",
    "        Context: {context}\n",
    "        Question: {question}\n",
    "        {format_instructions}\n",
    "        \"\"\"\n",
    "        # Set up a parser + inject instructions into the prompt template.\n",
    "        parser = JsonOutputParser(pydantic_object=Answer)\n",
    "        \n",
    "        # Instantiation using initializer\n",
    "        prompt = PromptTemplate(input_variables=[\"query\",\"context\"],\n",
    "                                partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "                                template=template)\n",
    "\n",
    "        # Set up RetrievalQA with the language model and retriever\n",
    "        qa_chain = RetrievalQA.from_chain_type(llm, \n",
    "                                            retriever=vectordb.as_retriever(search_kwargs={\"k\": 3}),\n",
    "                                            chain_type_kwargs= {\"prompt\": prompt}\n",
    "                                            )   \n",
    "    #---------------------------------------------------#\n",
    "    if include_prompt and not structured_result:\n",
    "        template = sysmessage + \"\"\"\n",
    "        Context: {context}\n",
    "        Question: {question}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Instantiation using initializer\n",
    "        prompt = PromptTemplate(input_variables=[\"query\",\"context\"],\n",
    "                                template=template)\n",
    "\n",
    "        # Set up RetrievalQA with the language model and retriever\n",
    "        qa_chain = RetrievalQA.from_chain_type(llm, \n",
    "                                            retriever=vectordb.as_retriever(search_kwargs={\"k\": 3}),\n",
    "                                            chain_type_kwargs= {\"prompt\": prompt}\n",
    "                                            )  \n",
    "    #---------------------------------------------------#\n",
    "    if not include_prompt and not structured_result:\n",
    "        # Set up RetrievalQA with the language model and retriever\n",
    "        qa_chain = RetrievalQA.from_chain_type(llm, \n",
    "                                            retriever=vectordb.as_retriever(search_kwargs={\"k\": 3}))\n",
    "        \n",
    "    #---------------------------------------------------#\n",
    "         \n",
    "    # Call the _call method\n",
    "    with get_openai_callback() as cb:\n",
    "        result = qa_chain({\"query\": question})        \n",
    "    \n",
    "    return result, cb    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, cb = qachatbot(question=\"Tell me about water sector in vietnam\",include_prompt=True, structured_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The water sector in Vietnam is experiencing growth, particularly in the residential segment. Volume growth is being supported by strong residential water demand, with projected 2023 volume growth for BWE at 7% YoY driven by 9% YoY residential water volume growth and 5% YoY industrial water volume growth. TDM is also expected to see volume growth in 2023 following its 2022 results that exceeded expectations. The water industry in Long An Province shows positive prospects for attracting FDI registrations and has a projected CAGR of 5% from 2020 to 2030, with industrial water demand estimated to double over the next 10 years.\n",
      "Related Question 1: What are the key drivers of growth in the water sector in Vietnam?\n",
      "Related Question 2: How does the water industry in Long An Province compare to other regions in Vietnam?\n",
      "Related Question 3: What impact does rapid urbanization and population growth have on the water sector in Vietnam?\n"
     ]
    }
   ],
   "source": [
    "# Assuming result[\"result\"] contains the JSON string\n",
    "result_json_string = result[\"result\"]\n",
    "\n",
    "# Parse the JSON string\n",
    "parsed_result = json.loads(result_json_string)\n",
    "\n",
    "# Access the parsed data\n",
    "answer = parsed_result[\"answer_\"]\n",
    "related_q1 = parsed_result[\"related_q1\"]\n",
    "related_q2 = parsed_result[\"related_q2\"]\n",
    "related_q3 = parsed_result[\"related_q3\"]\n",
    "\n",
    "# Print the parsed data\n",
    "print(\"Answer:\", answer)\n",
    "print(\"Related Question 1:\", related_q1)\n",
    "print(\"Related Question 2:\", related_q2)\n",
    "print(\"Related Question 3:\", related_q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
